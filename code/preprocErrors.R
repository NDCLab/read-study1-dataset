# Reading in reconciled Excels, summarizing the errors of each, and writing that
# to a new CSV
# Luc Sahar and Jessica M. Alexander -- NDCLab, Florida International University
# last updated 2024-03-13
# This version is intended for word-level operations

### --- IO information --- ###
##  INPUTS
#     This script relies on two external stateful components:
#       - scaffolds
#       - passage annotations
#
#     Scaffolds are stored in an Excel file with one sheet per passage. Each
#     sheet contains a list of each syllable and a list of each word, aligned
#     such that it is apparent which syllables belong to which word.
#     These are autogenerated by the genScaffolds script in the read-study1
#     repository.
#
#     For our purposes, this is important for calculating word-level errors (as
#     in "word stress errors", which we have considered to be errors that affect
#     whole words rather than individual errors), for determining whether errors
#     marked on adjacent syllables constitute discrete errors or just one error
#     (as in misproductions, which we here consider to be discrete when they
#     don't touch or when they touch at a word boundary, but not when they touch
#     inside a word), and for calculating rates of errors per syllable or per
#     word (the scaffolds make it simple to count the words or syllables that
#     each passage has).
#
#     Each passage annotation is a single Excel file consisting of rows with the
#     error types and two header rows---one for the passage's words, and another
#     below it for the passage's syllables. Below that, each cell thus
#     represents the presence or absence of an error of a particular type on a
#     particular syllable in the text. Because we're interested in calculations
#     by error type, we will transpose each annotation once it is read into a
#     dataframe, such that its columns are the error types and its rows are the
#     syllables.
#
#
##  PARAMETERS
#     This script uses a couple of repeat conventions. Hopefully it is as clear
#     and consistent as intended. Some clarifications:
#       -  path: a fully specified filesystem path to a file
#       -  dir: likewise, but for a directory (folder)
#       - "name": often used to distinguish whether we're talking about the text
#                 label -- the string identifying a file -- rather than the file
#                 itself or its contents
#       - df: dataframe, the R structure
#       - passage_df: a dataframe for a participant's reading of a given
#                 passage, with that passage's scaffolding included
#       - DEBUG_MODE: a 'flag' which when set to TRUE enables features including
#                 increased verbosity and incremental outputs (in case the
#                 program fails before creating the intended file per below)
#       - incremental_writeout: the name of the directory to which incremental
#                 outputs will periodically be saved in debug mode
#       - error_types_idiomatic: R-/dataframe- friendly names of the error types
#       - dict: a dictionary, used here for storing the scaffolds, the word
#               counts per passage, and the syllable counts per passage. This
#               prevents this from having to be recomputed for >1000 passages.
#               This is implemented as a new environment, but the idea of a
#               dictionary is familiar and the syntax of accessing an `env` in
#               R is similar to that of dictionaries in languages like Python.
#               Another option would be memoizing the function.
#       - lookbacks: columns appended to a df representing, at each row, the
#               value of some other column X rows back. This appears to be the
#               best way to perform sophisticated comparisons across errors over
#               a range of previous syllables.
#               For example, we might want to look for all misproductions,
#               insertions, and omissions occurring within four syllables of a
#               following hesitation or elongation---one way we might want to
#               look for potential cases of post-error slowing.
#
#     Everything else is intended to be as simple and self-explanatory as
#     possible. The spirit of the style is to have functions that are modular,
#     digestible, and whose titles are informative enough that reading the body
#     is typically unnecessary. Ideally, this makes code easier to understand
#     and to maintain.
#
#
##  OUTPUT
#     This script, when debug mode is not turned on (see above), writes only one
#     file: a massive CSV containing one row per participant per passage (there
#     are 20 rows per participant, assuming they read all passages).
#     Specific directories (rather than parameter names) are named with "base";
#     this was arbitrary.
#
#     That CSV is written to `outpath`, which is currently the concatenation of
#     the annotations root directory `annotations_base`, a somewhat useful
#     file name prefix (`label`), the current date and time (`timestamp`), and
#     the file extension '.csv'.
#
#     That file name is the last value returned if the script has run through in
#     its entirety. Other information is emitted by R along the way, along with
#     some other status updates when debug mode is turned on.
##
### --- IO information --- ###

DEBUG_MODE = TRUE

library(readxl) # read_xlsx
library(stringr) # str_extract
library(dplyr) # most things
library(purrr) # map, map_df; generally good to have
library(lubridate) # now
library(readr) # write_csv
library(glue) # glue
library(janitor) # make_clean_names

# setwd("your/path/to/this/repo")
# We need a directory with the gold standard for each of these passages:
path_to_read_dataset <- "." # usually equivalent to "../read-study1-dataset"

path_to_stimuli <-
  paste(path_to_read_dataset, "materials/reading-ranger/stimuli", sep = '/')

path_to_error_coding_excels <-
  paste(path_to_stimuli, "passage-error-excels", sep = '/')

path_to_resources <- paste(path_to_stimuli, 'resources', sep = '/')
# fs::dir_exists(path_to_error_coding_excels);
# fs::dir_exists(path_to_resources)

default_suffix <- "_reconciled" # for participant folder names

# if we're in debug mode, write output dataframes to disk as they are made
incremental_writeout <- if(DEBUG_MODE)
  paste(path_to_read_dataset, "incremental-passages_debugging", sep = '/') else
    NULL

if(DEBUG_MODE && !fs::is_dir(incremental_writeout))
  stop(
    paste("Intended debugging CSV directory (", incremental_writeout, ") not found. Try creating it?", sep = ""))
# fs::dir_create(incremental_writeout)


# ok we need our error types! how do we get this?
# copy from genScaffolds directly

# autogenerated scaffolds are located based on what rwe (readAloud) dataset does
scaffolds_path <- paste(path_to_read_dataset,
                        "code",
                        'scaffolds-autogenerated.xlsx',
                        sep = '/')
# ^don't use date, because we don't want competing standards for which scaffolds
# to use within a given project! just commit your changes if regenerating

stimuli_categories_from_subfolders <- function(dir_path) {
  # Given, say, three kinds of stimuli, denoted thanks to folder naming
  # conventions as dir_path/X, dir_path/Y, and dir_path/Z, give me the three
  # categories of stimuli: i.e. X, Y, and Z
  dir_path %>%
    fs::dir_ls(type = "directory") %>%
    map_vec(basename) %>%
    as.vector()
}

# get our categories of passages programmatically based on subfolders
stimuli_categories <-
  stimuli_categories_from_subfolders(path_to_error_coding_excels)

exemplar_excel_path <- path_to_error_coding_excels %>% # proof of concept: we CAN get it w/o hard coding
  paste(stimuli_categories[1], sep = '/') %>%
  fs::dir_ls() %>%
  last

exemplar_excel_df <- read_xlsx(exemplar_excel_path)

# we're progressing towards turning this "raw" excel df into something useful

sanitize <- function(word) { # helper
  # strip unwanted characters and downcase, but leave inflected
  unwanted="[,.;!?\"')(]"
  word %>%
    str_remove_all(unwanted) %>%
    tolower()
  # e.g.
  # test_scaffold %>% slice(1:50) %>% pull(word) %>% map_vec(sanitize)
}

check_value_by_col_index <- function(df, col_index, value) { # helper
  mutate(df, status = df[[col_index]] == value)
  # a boolean: in this same row, is the data in col #X equal to a fixed value Y?
}

drop_non_numeric_rows <- function(df) { # if, for a given row, a df's second column has non-0 data, drop that row
  df %>%
    check_value_by_col_index(2, 0) %>% # are the second col values 0?
    filter(status == TRUE) # only rows whose second column value is 0
  # The second col having values of 0 means that the first col is filled with
  # error types that are numeric (e.g. misproduction, hesitation, interference,
  # etc. rather than "actual correction produced" etc.)
}

# get idiomatic error types from row names
get_idiomatic_annotation_names <- function(passage_excel_df) {
  passage_excel_df %>%
    drop_non_numeric_rows() %>%
    pull(1) %>% # get first col i.e. error labels -- irrespective of col's name
    map_vec(make_clean_names)
}

annotation_type_names <- get_idiomatic_annotation_names(exemplar_excel_df)

read_error_data_from_path <- function(passage_path) {
  df <- # get the first row (syllable) plus all our numeric data
    passage_path %>%
    read_xlsx() %>%
    slice(0:length(annotation_type_names) + 1) %>%
    data.frame(row.names = append("syllable", annotation_type_names)) %>%
    t
  return(df[-1,] %>% # ignore the original titles
           as.data.frame) # and convert back to a dataframe
}


test_data_from_excel <- read_error_data_from_path(exemplar_excel_path)

# what we'll use when data is empty or invalid, until files are manually fixed
filler = data.frame(
  logical(length(annotation_type_names)), # ?? - RWE used 7, w/o documenting why
  row.names = annotation_type_names
) %>% t %>% as.data.frame



## Calculations about the passages themselves, for things like word ratios
titles <- excel_sheets(scaffolds_path)
timestamp = now("America/New_York") %>% format("%Y%m%d_%I%M%P")
# stim_char <- paste(base, 'materials/readAloud-ldt/stimuli/readAloud/readAloud-stimuli_characteristics.xlsx', sep="/", collapse=NULL)



## Set up frequency data
# downloaded on 06/13/2022 from
# https://www.ugent.be/pp/experimentele-psychologie/en/research/documents/subtlexus
path_to_subtlexus <-
  paste(path_to_resources, "SUBTLEXus74286wordstextversion.txt", sep="/")
# fs::is_file(path_to_subtlexus)

subtlexus <- read.table(path_to_subtlexus, header=TRUE)
subtlexus$Word <- tolower(subtlexus$Word) #make all entries in SUBTLEXUS lower-case

default_frequency <- # change from RWE! In prep, we set it to the corpus median;
                     # for simplicity, since we already use wordfreq data here,
                     # for READ instead we do it in preproc (here) rather than
                     # waiting til prep
  median(subtlexus$Lg10WF) #RWE preproc has it as 0 and corrects to this in prep

annotations_base <- paste(path_to_read_dataset,
                          "..",
                          "FOR-TESTING_reconciled-error-coding", #fixme
                          sep = '/')


get_frequency_for_word <- function(word) {
  # caution: if a lemma is desired, it must be derived elsewhere (this function
  # does not contain that logic)
  return(subtlexus$Lg10WF[match(word, subtlexus$Word)] %>%
           replace(is.na(.), default_frequency))
}


# programmatcially determine passage names
# fs::dir_ls(annotations_base, recurse = TRUE, type = "file") %>%
#   str_extract("([a-z]+)_([0-9]+g)", group = 1) %>%
#   unique() %>%
#   discard(is.na) %>%
#   sort  # %>% paste(collapse = "', '")
#
# fs::dir_ls(annotations_base, recurse = TRUE, type = "file") %>%
#   str_extract("([a-b]/[a-z]+)_([0-9]+g)", group = 1) %>%
#   unique() %>%
#   discard(is.na) %>%
#   sort %>% paste(collapse = "', '")

# Set up scaffolds and other passage-constant metadata
into_dict <- function(sequence, f, env = new.env()) {
  map(sequence, \(x) env[[x]] = f(x)) # fill a dictionary that maps x -> f(x)
  return(env)
}

# from genScaffolds; this is for testing purposes and circumvents hardcoding
filename_to_namespaced_label <- function(path) {# '|' as Excel disallows '/'
  ns <- path %>% dirname %>% basename # not the path itself but its parent
  qualify <- function(base) paste(ns, base, sep = '|') # prepend our ns

  path %>%
    basename %>%
    fs::path_ext_remove() %>%
    qualify

}
# filename_to_namespaced_label(exemplar_excel_path)

# First, we want to be able to quickly, easily, and expressively check, for any
# given passage, which group (a or b) that passage belongs to
# yield <- function(result) {
#   # give me a function that, irrespective of its arguments, returns result
#   function(...) result
# }


# membership <- into_dict(titles_a, \(.) "a")
# membership <- into_dict(titles_b, \(.) "b", env = membership)

tally_up <- function(df, col) # how many unique values in col?
  df %>% select({{col}}) %>% unique %>% nrow


# syntax: scaffolds[[passage_name]] -> scaffold df for that passage
scaffolds       = into_dict(titles, \(x)
                                    read_xlsx(scaffolds_path, sheet = x))

word_lists      = into_dict(titles, \(x)
                                    scaffolds[[x]] %>%
                                      distinct(word_id, .keep_all = TRUE) %>%
                                      pull(word_clean))

get_frequencies_for_passage <- function(nickname) # but must be namespaced!
  word_lists[[nickname]] %>% map_vec(get_frequency_for_word)

passage_frequencies = into_dict(titles, get_frequencies_for_passage)

# todo: scaffolds erroneously consider hyphenated words (ex. "long-haul") to be
# one word, for raw_word, word, *and* word_clean; figure out why and fix this

# test case:
sample_label <- filename_to_namespaced_label(exemplar_excel_path)
scaffolds[[sample_label]]
word_lists[[sample_label]]
passage_frequencies[[sample_label]]

## Now: logic to read in the error passage XLSXes
build_participant_dirname <- function(dir_root, participant_id, suffix = default_suffix) # github_root, 150077 -> "/home/[...]/sub-150077/sub-150077_reconciled"
  paste(sep = "",
        dir_root,
        '/sub-', participant_id,
        '/sub-', participant_id, suffix)


find_participant_dirname <- function(dir_root, participant_id) {
  dir_root %>%
    fs::dir_ls(recurse = TRUE,
               type = "dir",
               glob = glue("/sub-{participant_id}[^/*]/"))
}

find_participant_dirname <- function(dir_root, participant_id) {
  dir_root %>%
    fs::dir_map(identity, recurse = TRUE) %>%
    str_extract(glue(".*/sub-{participant_id}[^/]+/")) %>%
    discard(is.na) %>%
    unique()
}

find_participant_dirname(annotations_base, 3200002)


build_full_passage_path <- function(participant_dir_root, passage_nickname, grade) {
  participant_dir_root %>%
    paste('passage-set-', grade, membership[[passage_nickname]], '/', sep = '') %>%
    paste(passage_nickname, '_', grade, 'g.xlsx', sep = '')

} # %>% fs::is_file()

read_error_data_from_path <- function(passage_path) {
  df = data.frame(
    read_xlsx(passage_path)[2:13,], # get only the rows misprod ... corrected #fixme
    row.names = error_types_idiomatic # also isn't there already a whole thing for this?
  ) %>% t

  return(df[-1,] %>% # ignore the original titles
           as.data.frame) # and convert back to a dataframe
}


passage_name_to_df <- function(passage_nickname, participant_id, dir_root)
  cbind(scaffolds[[passage_nickname]], # include our scaffolds
        passage_nickname %>%
          build_full_passage_path(dir_root, participant_id) %>% # path
          read_error_data_from_path
  )

complain_when_invalid <- function(passage_df, participant_id, passage_nickname) {
  report = paste("\n\t\t<< ERROR REPORT", participant_id, "-", passage_nickname, ">>")

  any_empty = sum(is.na(passage_df)) != 0
  if (any_empty) {
    message(report); message("Empty value (NA) in the dataframe!\n")
    return(filler) # all values of FALSE
  }

  any_invalid = any(passage_df !=0 & passage_df != 1)
  if(any_invalid) {
    message(report); message("Invalid value (neither 1 nor 0) in the dataframe!\n")
    return(filler) # all values of FALSE
  }

  return(passage_df)
}


collapse_by_word <- function(passage_df) {
  # for each word in the passage, consider it an error when any of its syllables
  # is marked as such

  # map over words
  # is any syllable corresponding to this word marked?
  # if yes TRUE
  # else FALSE

  passage_df %>%
    mutate(across(misprod:corrected, # fixme
                  ~ as.logical(as.integer(.)))) %>% # make booleans
    group_by(word_id) %>%                           # collapse as words
    summarize(across(misprod:corrected, ~ any(.)))  # error on this word or not?
}

append_words_and_frequencies <- function(collapsed_df, nickname)
  cbind(word           = word_lists[[nickname]],
        log10frequency = passage_frequencies[[nickname]],
        collapsed_df)

colnames_from_range <- function(df, colrange)
  colnames(select(df, {{colrange}}))

append_lookback <- function(df, col, lookback_index)
  # Add a new column (e.g. prev_misprod4) representing the value of e.g. misprod, four rows prior.
  # This is useful for hunting for patterns of errors in a particular sequence
  mutate(df,
         "prev_{col}{lookback_index}" := lag(df[[col]], n = lookback_index))

append_lookback_multicol <- function(df, colrange, lookback_index) {
  # for every column in passed range, create a new column looking back at the indexth row for that column
  col_list = colnames_from_range(df, {{colrange}})

  reduce(col_list,
         \(df_acc, colname) append_lookback(df_acc, {{colname}}, lookback_index),
         .init = df)
}

append_lookbacks_multicol <- function(df, colrange, lookback_count)
  # Use append_lookback_multicol successively on the same df, on each index from 1..lookback_count
  # Ex: col=hesitation, lookback_count=3 will add a column for hesitations three rows prior,
  # another for hesitations two rows prior, and another for hesitations one row prior
  reduce(1:lookback_count,
         partial(append_lookback_multicol, colrange = {{colrange}}),
         .init = df)


a_b_sequence_lookback <- function(df, errtypes_a, errtypes_b, prior_context = 1) {
  # a: LHS errors; b: RHS errors; context: how many rows back we can look for LHS errors

  lhs_cols = colnames_from_range(df, {{errtypes_a}})
  rhs_cols = colnames_from_range(df, {{errtypes_b}})

  lookbacks_regex = lhs_cols %>% paste(collapse = "|") %>% paste("prev_(", ., ").*", sep = "") # as in prev_(misprod|hesitation).*

  df_with_lhs_lookbacks = append_lookbacks_multicol(df, {{errtypes_a}}, prior_context)


  mutate(df_with_lhs_lookbacks,
         "any_prior_{{errtypes_a}}" := if_any(matches(lookbacks_regex), ~ . == 1),
         "{{errtypes_b}}_with_any_prior_{{errtypes_a}}" := if_any(rhs_cols, ~ . == 1) & if_any(matches(lookbacks_regex), ~ . == 1))
}

# All the above, with lookaheads
append_lookahead <- function(df, col, lookahead_index)
  # Add a new column (e.g. next_misprod4) representing the value of e.g. misprod, four rows ahead.
  # This is useful for hunting for patterns of errors in a particular sequence
  mutate(df,
         "next_{col}{lookahead_index}" := lead(df[[col]], n = lookahead_index))

append_lookahead_multicol <- function(df, colrange, lookahead_index) {
  # for every column in passed range, create a new column looking forward at the indexth row for that column
  col_list = colnames_from_range(df, {{colrange}})

  reduce(col_list,
         \(df_acc, colname) append_lookahead(df_acc, {{colname}}, lookahead_index),
         .init = df)
}

append_lookaheads_multicol <- function(df, colrange, lookahead_count)
  # Use append_lookahead_multicol successively on the same df, on each index from 1..lookahead_count
  # Ex: col=hesitation, lookahead_count=3 will add a column for hesitations three rows ahead,
  # another for hesitations two rows ahead, and another for hesitations one row ahead
  reduce(1:lookahead_count,
         partial(append_lookahead_multicol, colrange = {{colrange}}),
         .init = df)


a_b_sequence_lookahead <- function(df, errtypes_a, errtypes_b, forward_context = 1) {
  # a: LHS errors; b: RHS errors; context: how many rows forward we can look for RHS errors

  lhs_cols = colnames_from_range(df, {{errtypes_a}})
  rhs_cols = colnames_from_range(df, {{errtypes_b}})

  lookaheads_regex = rhs_cols %>% paste(collapse = "|") %>% paste("next_(", ., ").*", sep = "") # as in next_(misprod|hesitation).*

  df_with_rhs_lookaheads = append_lookaheads_multicol(df, {{errtypes_b}}, forward_context)

  mutate(df_with_rhs_lookaheads,
         "any_upcoming_{{errtypes_b}}" := if_any(matches(lookaheads_regex), ~ . == 1),
         "{{errtypes_a}}_with_any_upcoming_{{errtypes_b}}" := if_any(lhs_cols, ~ . == 1) & if_any(matches(lookaheads_regex), ~ . == 1)
  )
}

# we just want from lookaheads and lookbacks:
# was there ANY in previous 5
# psuedocode: any(prev_*)
# likewise for next

# & all of this for misprod->hes and hes->misprod

# so we add four columns:
# is it a misprod with preceding hes?
# is it a misprod with following hes?
# is it a hes with preceding misprod?
# is it a hes with following misprod?

append_pes_annotation_cols <- function(df, errtypes_a, errtypes_b, forward_context = 1, prior_context = 1) {
  # a: LHS errors; b: RHS errors; context: how many rows forward we can look for RHS errors

  lhs_cols = colnames_from_range(df, {{errtypes_a}})
  rhs_cols = colnames_from_range(df, {{errtypes_b}})

  lookaheads_regex = rhs_cols %>% paste(collapse = "|") %>% paste("next_(", ., ").*", sep = "") # as in next_(misprod|hesitation).*
  df_with_rhs_lookaheads = append_lookaheads_multicol(df, {{errtypes_b}}, forward_context)

  lookbacks_regex = lhs_cols %>% paste(collapse = "|") %>% paste("prev_(", ., ").*", sep = "") # as in prev_(misprod|hesitation).*
  df_with_lhs_lookbacks = append_lookbacks_multicol(df, {{errtypes_a}}, prior_context)

  df_with_rhs_lookaheads_and_lhs_lookbacks =
    df %>%
    a_b_sequence_lookahead({{errtypes_a}}, {{errtypes_b}}, forward_context) %>%
    a_b_sequence_lookback({{errtypes_a}}, {{errtypes_b}}, prior_context)

  return(df_with_rhs_lookaheads_and_lhs_lookbacks)
}

status_message <- function(passage_name, participant_id) {
  status = paste("Generating word errors from participant ", participant_id, "'s ",
                 passage_name, " passage...",
                 sep = '')
  message(status)
}

error_summary_with_metadata <- function(passage_nickname, participant_id, dir_root) {
  if(DEBUG_MODE) status_message(passage_nickname, participant_id)

  summary =
    passage_name_to_df(passage_nickname, participant_id, dir_root) %>%
    # complain_when_invalid(participant_id, passage_nickname) %>%
    collapse_by_word %>%
    append_words_and_frequencies(passage_nickname) %>%
    append_pes_annotation_cols(misprod, hesitation, forward_context = 5, prior_context =  5) %>%
    append_pes_annotation_cols(hesitation, misprod, forward_context = 5, prior_context =  5) %>%
    select(word:corrected, matches("(^|.*_)any.*[_$]")) # any_prior_misprod, any_following_hesitation, any_prior_hesitation, any_following_misprod)

  return(cbind(
    id = participant_id, # pre-pose an id column
    passage = passage_nickname, # then a passage column
    summary
  ))
}

# All passages for a participant
generate_summary_for_each_passage_with_metadata <- function(dir_root, participant_id, write_to = incremental_writeout) {
  df =
    build_participant_dirname(dir_root, participant_id) %>% # identify their folder
    dir %>% # pick out: which passages did they actually read?
    fs::path_ext_remove() %>% # take them _without_ the extension
    map_df(error_summary_with_metadata, participant_id, dir_root) # glue that together into a df

  if (!is.null(write_to) && fs::is_dir(write_to)) { # incremental CSV: just this participant, with all their passages
    outfile_debug = paste(write_to, "/", participant_id, "_", timestamp, '.csv', sep = "")
    write_csv(df, outfile_debug)
  }

  return(df)
}

# Now, for each participant under a directory, each identified by the form sub_XXXXXX_reconciled,
# call generate_summary_for_each_passage_with_metadata(the_parentdir_of_all_those, that_id)
summarize_errors_in_subdirectories <- function(dir_root, subfolder_match)
  dir_root %>%
  dir(include.dirs = TRUE, recursive = TRUE, pattern = subfolder_match) %>% # walk the directory
  map(\(dir) str_extract(dir,  "\\d+")) %>% # split them up: sub-150079_reconciled -> 150079
  map_df(generate_summary_for_each_passage_with_metadata, dir_root = dir_root) # summarize all spreadsheets for that participant, for each participant

# we've matched subfolders by explicitly returning directories (include.dirs =
# TRUE) and recursing (recursive = TRUE), thus catching -"_reconciled" subfolders

## Now finally: write all our results to a file (a CSV)
annotations_base = paste(base, "derivatives/preprocessed", sep = '/')
github_root = paste(annotations_base, "error-coding", sep = '/')

label = "disfluencies_subject-x-passage-x-word_"

outpath <- paste(sep = "", annotations_base, '/', label, timestamp, ".csv")
# e.g. "./some/path/disfluencies_20230520_1240pm.csv"

github_root %>% # passage_dir %>%
  summarize_errors_in_subdirectories("^sub-\\d{6}_reconciled$") %>%
  write_csv(outpath)

print(outpath)
