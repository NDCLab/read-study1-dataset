# Reading in reconciled Excels, summarizing the errors of each, and writing that
# to a new CSV
# Luc Sahar and Jessica M. Alexander -- NDCLab, Florida International University
# last updated 2024-03-15
# This version is intended for word-level operations

### --- IO information --- ###
##  INPUTS
#     This script relies on two external stateful components:
#       - scaffolds
#       - passage annotations
#
#     Scaffolds are stored in an Excel file with one sheet per passage. Each
#     sheet contains a list of each syllable and a list of each word, aligned
#     such that it is apparent which syllables belong to which word.
#     These are autogenerated by the genScaffolds script in the read-study1
#     repository.
#
#     For our purposes, this is important for calculating word-level errors (as
#     in "word stress errors", which we have considered to be errors that affect
#     whole words rather than individual errors), for determining whether errors
#     marked on adjacent syllables constitute discrete errors or just one error
#     (as in misproductions, which we here consider to be discrete when they
#     don't touch or when they touch at a word boundary, but not when they touch
#     inside a word), and for calculating rates of errors per syllable or per
#     word (the scaffolds make it simple to count the words or syllables that
#     each passage has).
#
#     Each passage annotation is a single Excel file consisting of rows with the
#     error types and two header rows---one for the passage's words, and another
#     below it for the passage's syllables. Below that, each cell thus
#     represents the presence or absence of an error of a particular type on a
#     particular syllable in the text. Because we're interested in calculations
#     by error type, we will transpose each annotation once it is read into a
#     dataframe, such that its columns are the error types and its rows are the
#     syllables.
#
#
##  PARAMETERS
#     This script uses a couple of repeat conventions. Hopefully it is as clear
#     and consistent as intended. Some clarifications:
#       -  path: a fully specified filesystem path to a file
#       -  dir: likewise, but for a directory (folder)
#       - "name": often used to distinguish whether we're talking about the text
#                 label -- the string identifying a file -- rather than the file
#                 itself or its contents
#       - df: dataframe, the R structure
#       - passage_df: a dataframe for a participant's reading of a given
#                 passage, with that passage's scaffolding included
#       - DEBUG_MODE: a 'flag' which when set to TRUE enables features including
#                 increased verbosity and incremental outputs (in case the
#                 program fails before creating the intended file per below)
#       - incremental_writeout: the name of the directory to which incremental
#                 outputs will periodically be saved in debug mode
#       - error_types_idiomatic: R-/dataframe- friendly names of the error types
#       - dict: a dictionary, used here for storing the scaffolds, the word
#               counts per passage, and the syllable counts per passage. This
#               prevents this from having to be recomputed for >1000 passages.
#               This is implemented as a new environment, but the idea of a
#               dictionary is familiar and the syntax of accessing an `env` in
#               R is similar to that of dictionaries in languages like Python.
#               Another option would be memoizing the function.
#       - lookbacks: columns appended to a df representing, at each row, the
#               value of some other column X rows back. This appears to be the
#               best way to perform sophisticated comparisons across errors over
#               a range of previous syllables.
#               For example, we might want to look for all misproductions,
#               insertions, and omissions occurring within four syllables of a
#               following hesitation or elongation---one way we might want to
#               look for potential cases of post-error slowing.
#
#     Everything else is intended to be as simple and self-explanatory as
#     possible. The spirit of the style is to have functions that are modular,
#     digestible, and whose titles are informative enough that reading the body
#     is typically unnecessary. Ideally, this makes code easier to understand
#     and to maintain.
#
#
##  OUTPUT
#     This script, when debug mode is not turned on (see above), writes only one
#     file: a massive CSV containing one row per participant per passage (there
#     are 20 rows per participant, assuming they read all passages).
#     Specific directories (rather than parameter names) are named with "base";
#     this was arbitrary.
#
#     That CSV is written to `outpath`, which is currently the concatenation of
#     the annotations root directory `annotations_base`, a somewhat useful
#     file name prefix (`label`), the current date and time (`timestamp`), and
#     the file extension '.csv'.
#
#     That file name is the last value returned if the script has run through in
#     its entirety. Other information is emitted by R along the way, along with
#     some other status updates when debug mode is turned on.
##
### --- IO information --- ###

DEBUG_MODE = TRUE

library(readxl) # read_xlsx
library(stringr) # str_extract
library(dplyr) # most things
library(purrr) # map, map_df; generally good to have
library(lubridate) # now
library(readr) # write_csv
library(glue) # glue
library(janitor) # make_clean_names

# setwd("your/path/to/this/repo")
# We need a directory with the gold standard for each of these passages:
path_to_read_dataset <- "." # usually equivalent to "../read-study1-dataset"

path_to_stimuli <-
  paste(path_to_read_dataset, "materials/reading-ranger/stimuli", sep = '/')

path_to_error_coding_excels <-
  paste(path_to_stimuli, "passage-error-excels", sep = '/')

path_to_resources <- paste(path_to_stimuli, 'resources', sep = '/')
# fs::dir_exists(path_to_error_coding_excels);
# fs::dir_exists(path_to_resources)

default_suffix <- "_reconciled" # for participant folder names

# if we're in debug mode, write output dataframes to disk as they are made
incremental_writeout <- if(DEBUG_MODE)
  paste(path_to_read_dataset, "incremental-passages_debugging", sep = '/') else
    NULL

if(DEBUG_MODE && !fs::is_dir(incremental_writeout))
  stop(
    paste("Intended debugging CSV directory (", incremental_writeout, ") not found. Try creating it?", sep = ""))
# fs::dir_create(incremental_writeout)


# ok we need our error types! how do we get this?
# copy from genScaffolds.R directly

# autogenerated scaffolds are located based on what rwe (readAloud) dataset does
scaffolds_path <- paste(path_to_read_dataset,
                        "code",
                        'scaffolds-autogenerated.xlsx',
                        sep = '/')
# ^don't use date, because we don't want competing standards for which scaffolds
# to use within a given project! just commit your changes if regenerating

stimuli_categories_from_subfolders <- function(dir_path) {
  # Given, say, three kinds of stimuli, denoted thanks to folder naming
  # conventions as dir_path/X, dir_path/Y, and dir_path/Z, give me the three
  # categories of stimuli: i.e. X, Y, and Z
  dir_path %>%
    fs::dir_ls(type = "directory") %>%
    map_vec(basename) %>%
    as.vector()
}

# get our categories of passages programmatically based on subfolders
stimuli_categories <-
  stimuli_categories_from_subfolders(path_to_error_coding_excels)

exemplar_excel_path <- path_to_error_coding_excels %>% # proof of concept: we CAN get it w/o hard coding
  paste(stimuli_categories[1], sep = '/') %>%
  fs::dir_ls() %>%
  last # NB this is blank, uncoded

exemplar_excel_df <- read_xlsx(exemplar_excel_path)

# we're progressing towards turning this "raw" excel df into something useful

sanitize <- function(word) { # helper
  # strip unwanted characters and downcase, but leave inflected
  unwanted="[,.;!?\"')(]"
  word %>%
    str_remove_all(unwanted) %>%
    tolower()
  # e.g.
  # test_scaffold %>% slice(1:50) %>% pull(word) %>% map_vec(sanitize)
}

check_value_by_col_index <- function(df, col_index, value) { # helper
  mutate(df, status = df[[col_index]] == value)
  # a boolean: in this same row, is the data in col #X equal to a fixed value Y?
}

drop_non_numeric_rows <- function(df) { # if, for a given row, a df's second column has non-0 data, drop that row
  df %>%
    check_value_by_col_index(2, 0) %>% # are the second col values 0?
    filter(status == TRUE) # only rows whose second column value is 0
  # The second col having values of 0 means that the first col is filled with
  # error types that are numeric (e.g. misproduction, hesitation, interference,
  # etc. rather than "actual correction produced" etc.)
}

# get idiomatic error types from row names
get_idiomatic_annotation_names <- function(passage_excel_df) {
  passage_excel_df %>%
    drop_non_numeric_rows() %>%
    pull(1) %>% # get first col i.e. error labels -- irrespective of col's name
    map_vec(make_clean_names)
}

annotation_type_names <- get_idiomatic_annotation_names(exemplar_excel_df)

read_error_data_from_path <- function(passage_path) {
  df <- # get the first row (syllable) plus all our numeric data
    passage_path %>%
    read_xlsx() %>%
    slice(0:length(annotation_type_names) + 1) %>%
    data.frame(row.names = append("syllable", annotation_type_names)) %>%
    t
  return(df[-1,] %>% # ignore the original titles
           as.data.frame) # and convert back to a dataframe
}


test_data_from_excel <- read_error_data_from_path(exemplar_excel_path)

# what we'll use when data is empty or invalid, until files are manually fixed
filler = data.frame(
  logical(length(annotation_type_names)), # ?? - RWE used 7, w/o documenting why
  row.names = annotation_type_names
) %>% t %>% as.data.frame



## Calculations about the passages themselves, for things like word ratios
titles <- excel_sheets(scaffolds_path)
timestamp <- now("America/New_York") %>% format("%Y%m%d_%I%M%P")
# stim_char <- paste(base, 'materials/readAloud-ldt/stimuli/readAloud/readAloud-stimuli_characteristics.xlsx', sep="/", collapse=NULL)



## Set up frequency data
# downloaded on 06/13/2022 from
# https://www.ugent.be/pp/experimentele-psychologie/en/research/documents/subtlexus
path_to_subtlexus <-
  paste(path_to_resources, "SUBTLEXus74286wordstextversion.txt", sep="/")
# fs::is_file(path_to_subtlexus)

subtlexus <- read.table(path_to_subtlexus, header=TRUE)
subtlexus$Word <- tolower(subtlexus$Word) #make all entries in SUBTLEXUS lower-case

default_frequency <- # change from RWE! In prep, we set it to the corpus median;
                     # for simplicity, since we already use wordfreq data here,
                     # for READ instead we do it in preproc (here) rather than
                     # waiting til prep
  median(subtlexus$Lg10WF) #RWE preproc has it as 0 and corrects to this in prep

annotations_base <- paste(path_to_read_dataset,
                          "..",
                          "FOR-TESTING_reconciled-error-coding-corrected-structure", #fixme
                          sep = '/')

exemplar_coded_excel_path <- annotations_base %>% # proof of concept: we CAN get it w/o hard coding
  fs::dir_ls(type = "dir") %>% # all participants
  first %>%                    # first participant
  paste(stimuli_categories[1], sep = '/') %>%
  fs::dir_ls() %>%
  last

test_coded_data_from_excel <- read_error_data_from_path(exemplar_coded_excel_path)

get_frequency_for_word <- function(word) {
  # caution: if a lemma is desired, it must be derived elsewhere (this function
  # does not contain that logic)
  return(subtlexus$Lg10WF[match(word, subtlexus$Word)] %>%
           replace(is.na(.), default_frequency))
}

# Set up scaffolds and other passage-constant metadata
into_dict <- function(sequence, f, env = new.env()) {
  map(sequence, \(x) env[[x]] = f(x)) # fill a dictionary that maps x -> f(x)
  return(env)
}

# from genScaffolds; this is for testing purposes and circumvents hardcoding
# Given the full path to a coded Excel for a participant's reading of one passage,
# uniquely identify *which* passage - and which version - was read
filename_to_namespaced_label <- function(path) {# '|' as Excel disallows '/'
  ns <- path %>% dirname %>% basename # not the path itself but its parent
  qualify <- function(base) paste(ns, base, sep = '|') # prepend our ns

  path %>%
    basename %>%
    fs::path_ext_remove() %>%
    qualify

}
# filename_to_namespaced_label(exemplar_excel_path)

dirname_to_participant_id <- function(path) {
  str_extract(path,  "\\d+") # split them up: sub-150079_reconciled -> 150079
}

# First, we want to be able to quickly, easily, and expressively check, for any
# given passage, which group (a or b) that passage belongs to
# yield <- function(result) {
#   # give me a function that, irrespective of its arguments, returns result
#   function(...) result
# }


# membership <- into_dict(titles_a, \(.) "a")
# membership <- into_dict(titles_b, \(.) "b", env = membership)

tally_up <- function(df, col) # how many unique values in col?
  df %>% select({{col}}) %>% unique %>% nrow


# syntax: scaffolds[[passage_name]] -> scaffold df for that passage
scaffolds       = into_dict(titles, \(x)
                                    read_xlsx(scaffolds_path, sheet = x) %>%
                                      filter(!is.na(syllable)))

word_lists      = into_dict(titles, \(x)
                                    scaffolds[[x]] %>%
                                      distinct(word_id, .keep_all = TRUE) %>%
                                      pull(word_clean))

# get_frequencies_for_passage <- function(nickname) # but must be namespaced!
#   word_lists[[nickname]] %>% map_vec(get_frequency_for_word)
get_frequencies_for_passage <- function(nickname) # but must be namespaced!
  scaffolds[[nickname]] %>% pull(word_clean) %>% map_vec(get_frequency_for_word)



passage_frequencies = into_dict(titles, get_frequencies_for_passage)

# todo: scaffolds erroneously consider hyphenated words (ex. "long-haul") to be
# one word, for raw_word, word, *and* word_clean; figure out why and fix this

# test case:
sample_label <- filename_to_namespaced_label(exemplar_excel_path)
scaffolds[[sample_label]]
word_lists[[sample_label]]
passage_frequencies[[sample_label]]

## Now: logic to process the error passage XLSXes into end result

a_b_column_sets_to_bidirectionally_check_for <- # warning: not general!
  data.frame(
    lhs = c("hesitation", "misproduced_syllable"), # check this
    rhs = c("misproduced_syllable", "hesitation")  # against this
  )

conjoin_passage_metadata_with_coded_data <- function(error_data_df,
                                                     passage_label) {
  # a join would be better but we aren't tracking syllable identity when reading
  # in coded error data (as would be required); a join by syllable text alone
  # would be meaningless
  bind_cols(scaffolds[[passage_label]],
            error_data_df %>%
              filter(!is.na(syllable)) %>% # best shot at making them same len
              select(-syllable)) %>% # it already exists in scaffolds
    mutate(wordFreq = passage_frequencies[[passage_label]])
}

# test case:
conjoin_a <-
  bind_cols(scaffolds[[sample_label]],
            test_coded_data_from_excel %>%
            select(-syllable)) %>%
  mutate(wordFreq = passage_frequencies[[sample_label]])

conjoin_b <-
  conjoin_passage_metadata_with_coded_data(test_coded_data_from_excel,
                                           sample_label)
all.equal(conjoin_a, conjoin_b)

complain_when_invalid <- function(passage_df, participant_id, passage_nickname) {
  report = paste("\n\t\t<< ERROR REPORT", participant_id, "-", passage_nickname, ">>")

  any_empty = sum(is.na(passage_df)) != 0
  if (any_empty) {
    message(report); message("Empty value (NA) in the dataframe!\n")
    return(filler) # all values of FALSE
  }

  any_invalid = any(passage_df !=0 & passage_df != 1)
  if(any_invalid) {
    message(report); message("Invalid value (neither 1 nor 0) in the dataframe!\n")
    return(filler) # all values of FALSE
  }

  return(passage_df)
}


collapse_by_word <- function(passage_df) {
  # for each word in the passage, consider it an error when **any** of its
  # syllables is marked as such

  # map over words
  # is any syllable corresponding to this word marked?
  # if yes TRUE
  # else FALSE
  # then we don't need entries beyond the first for each word id, so we drop them

  passage_df %>%
    mutate(across(annotation_type_names, # replace 1s and 0s with TRUE and FALSE
                  ~ as.logical(as.integer(.)))) %>%
    group_by(passage, category, word_id) %>%
    reframe(across(-annotation_type_names, identity), # leave non errors as is
            across(annotation_type_names, ~ any(.))) %>% # flatten errors to word level
    distinct(word_id, .keep_all = TRUE) %>% # dedup
    arrange(ordered_word_id = extract_end_of_word_id(word_id)) # original order

}

# let's test on this
conjoin_test <- conjoin_b %>% mutate(across(annotation_type_names,
                                            ~ as.logical(as.integer(.))))

extract_end_of_word_id <- function(full_id) {
  str_extract(full_id, "\\d+$") %>% as.integer()
}

# those which are not meaningful at the final word level
syllable_dependent_cols <- c("syllable", "wordOnset", "syllable_id", "raw_word")

conjoin_test %>%
  group_by(word_id) %>%
  reframe(across(-annotation_type_names, identity), # leave non errors as is
          across(annotation_type_names, ~ any(.))) %>% # flatten errors to word level
  # select(!any_of(syllable_dependent_cols)) %>%
  distinct(word_id, .keep_all = TRUE)


exemplar_participant_id <-
  exemplar_coded_excel_path %>% dirname %>% dirname_to_participant_id


colnames_from_range <- function(df, colrange)
  colnames(select(df, {{colrange}}))

append_lookback <- function(df, col, lookback_index)
  # Add a new column (e.g. prev_misprod4) representing the value of e.g. misprod, four rows prior.
  # This is useful for hunting for patterns of errors in a particular sequence
  mutate(df,
         "prev_{col}{lookback_index}" := lag(df[[col]], n = lookback_index))

append_lookback_multicol <- function(df, colrange, lookback_index) {
  # for every column in passed range, create a new column looking back at the indexth row for that column
  col_list = colnames_from_range(df, {{colrange}})

  reduce(col_list,
         \(df_acc, colname) append_lookback(df_acc, {{colname}}, lookback_index),
         .init = df)
}

append_lookbacks_multicol <- function(df, colrange, lookback_count)
  # Use append_lookback_multicol successively on the same df, on each index from 1..lookback_count
  # Ex: col=hesitation, lookback_count=3 will add a column for hesitations three rows prior,
  # another for hesitations two rows prior, and another for hesitations one row prior
  reduce(1:lookback_count,
         partial(append_lookback_multicol, colrange = {{colrange}}),
         .init = df)


a_b_sequence_lookback <- function(df, errtypes_a, errtypes_b, prior_context = 1) {
  # a: LHS errors; b: RHS errors; context: how many rows back we can look for LHS errors

  lhs_cols = colnames_from_range(df, {{errtypes_a}})
  rhs_cols = colnames_from_range(df, {{errtypes_b}})

  lookbacks_regex = lhs_cols %>% paste(collapse = "|") %>% paste("prev_(", ., ").*", sep = "") # as in prev_(misprod|hesitation).*

  df_with_lhs_lookbacks = append_lookbacks_multicol(df, {{errtypes_a}}, prior_context)

  col1 = paste("any_prior", lhs_cols, collapse = "_", sep = '_') # for labeling columns
  col2 = paste(rhs_cols, "with", col1, collapse = "_", sep = '_') # for labeling columns

  mutate(df_with_lhs_lookbacks,
         {{col1}} := if_any(matches(lookbacks_regex), ~ . == 1),
         {{col2}} := if_any(rhs_cols, ~ . == 1) & if_any(matches(lookbacks_regex), ~ . == 1))
}

# All the above, with lookaheads
append_lookahead <- function(df, col, lookahead_index)
  # Add a new column (e.g. next_misprod4) representing the value of e.g. misprod, four rows ahead.
  # This is useful for hunting for patterns of errors in a particular sequence
  mutate(df,
         "next_{col}{lookahead_index}" := lead(df[[col]], n = lookahead_index))

append_lookahead_multicol <- function(df, colrange, lookahead_index) {
  # for every column in passed range, create a new column looking forward at the indexth row for that column
  col_list = colnames_from_range(df, {{colrange}})

  reduce(col_list,
         \(df_acc, colname) append_lookahead(df_acc, {{colname}}, lookahead_index),
         .init = df)
}

append_lookaheads_multicol <- function(df, colrange, lookahead_count)
  # Use append_lookahead_multicol successively on the same df, on each index from 1..lookahead_count
  # Ex: col=hesitation, lookahead_count=3 will add a column for hesitations three rows ahead,
  # another for hesitations two rows ahead, and another for hesitations one row ahead
  reduce(1:lookahead_count,
         partial(append_lookahead_multicol, colrange = {{colrange}}),
         .init = df)


a_b_sequence_lookahead <- function(df, errtypes_a, errtypes_b, forward_context = 1) {
  # a: LHS errors; b: RHS errors; context: how many rows forward we can look for RHS errors

  lhs_cols = colnames_from_range(df, {{errtypes_a}})
  rhs_cols = colnames_from_range(df, {{errtypes_b}})

  lookaheads_regex = rhs_cols %>% paste(collapse = "|") %>% paste("next_(", ., ").*", sep = "") # as in next_(misprod|hesitation).*

  df_with_rhs_lookaheads = append_lookaheads_multicol(df, {{errtypes_b}}, forward_context)

  col1 = paste("any_upcoming", rhs_cols, collapse = "_", sep = '_') # for labeling columns
  col2 = paste(lhs_cols, "with", col1, collapse = "_", sep = '_') # for labeling columns

  mutate(df_with_rhs_lookaheads,
         {{col1}} := if_any(matches(lookaheads_regex), ~ . == 1),
         {{col2}} := if_any(lhs_cols, ~ . == 1) & if_any(matches(lookaheads_regex), ~ . == 1)
  )
}

# we just want from lookaheads and lookbacks:
# was there ANY in previous 5
# psuedocode: any(prev_*)
# likewise for next

# & all of this for misprod->hes and hes->misprod

# so we add four columns:
# is it a misprod with preceding hes?
# is it a misprod with following hes?
# is it a hes with preceding misprod?
# is it a hes with following misprod?

append_pes_annotation_cols <- function(df, errtypes_a, errtypes_b, forward_context = 1, prior_context = 1) {
  # a: LHS errors; b: RHS errors; context: how many rows forward we can look for RHS errors

  lhs_cols = colnames_from_range(df, {{errtypes_a}})
  rhs_cols = colnames_from_range(df, {{errtypes_b}})

  lookaheads_regex = rhs_cols %>% paste(collapse = "|") %>% paste("next_(", ., ").*", sep = "") # as in next_(misprod|hesitation).*
  df_with_rhs_lookaheads = append_lookaheads_multicol(df, {{errtypes_b}}, forward_context)

  lookbacks_regex = lhs_cols %>% paste(collapse = "|") %>% paste("prev_(", ., ").*", sep = "") # as in prev_(misprod|hesitation).*
  df_with_lhs_lookbacks = append_lookbacks_multicol(df, {{errtypes_a}}, prior_context)

  df_with_rhs_lookaheads_and_lhs_lookbacks =
    df %>%
    a_b_sequence_lookahead({{errtypes_a}}, {{errtypes_b}}, forward_context) %>%
    a_b_sequence_lookback({{errtypes_a}}, {{errtypes_b}}, prior_context)

  return(df_with_rhs_lookaheads_and_lhs_lookbacks)
}

status_message <- function(passage_name, participant_id) { # fixme: path, not sep args
  status = paste("Generating word errors from participant ", participant_id, "'s ",
                 passage_name, " passage...",
                 sep = '')
  message(status)
}

# One participant's summary for one passage based on one file for the
# annotations on their reading of that one passage
error_summary_with_metadata <- function(passage_path) {
  passage_nickname <- filename_to_namespaced_label(passage_path)
  participant_id   <- passage_path %>% dirname %>% dirname_to_participant_id

  if(DEBUG_MODE) status_message(passage_nickname, participant_id)

  summary = # this whole fn should just take a file name instead!
    passage_path %>%
    read_error_data_from_path %>%
  # complain_when_invalid(participant_id, passage_nickname) %>%
    conjoin_passage_metadata_with_coded_data(passage_nickname) %>%
    collapse_by_word %>%
    append_pes_annotation_cols(forward_context = 5, prior_context = 5,
      a_b_column_sets_to_bidirectionally_check_for$lhs[1],
      a_b_column_sets_to_bidirectionally_check_for$rhs[1]) %>%
    append_pes_annotation_cols(forward_context = 5, prior_context = 5,
      a_b_column_sets_to_bidirectionally_check_for$lhs[2],
      a_b_column_sets_to_bidirectionally_check_for$rhs[2]) %>%
    select(-matches("(prev|next)_*")) %>% # drop incremental a_b cols
    select(-syllable_dependent_cols)      # and those unmeaningful at word level

  return(mutate(summary, participant_id = participant_id, .before = 1))

}

exemplar_error_summary <- error_summary_with_metadata(exemplar_coded_excel_path)


# All passages for a participant
generate_summary_for_each_passage_with_metadata <- function(participant_dir, write_to = incremental_writeout) {
  participant_id <- dirname_to_participant_id(participant_dir)
  df =
    participant_dir %>% # identify their folder
    fs::dir_ls(recurse = TRUE, type = "file") %>% # pick out: which passages did they actually read?
    str_subset(glue("sub-{participant_id}[^/]*.xlsx"), negate = TRUE) %>%
    map_df(error_summary_with_metadata) # glue that together into a df

  if (!is.null(write_to) && fs::is_dir(write_to)) { # incremental CSV: just this participant, with all their passages
    outfile_debug = paste(write_to, "/", participant_id, "_", timestamp, '.csv', sep = "")
    write_csv(df, outfile_debug)
  }

  return(df)
}

exemplar_participant_dir <-
  annotations_base %>% # proof of concept: we CAN get it w/o hard coding
  fs::dir_ls(type = "dir") %>% # all participants
  first

exemplar_participant_dir %>% fs::dir_ls(recurse = TRUE, type = "file")

exemplar_summary_whole_participant <-
  exemplar_participant_dir %>%
  generate_summary_for_each_passage_with_metadata

# Now, for each participant under a directory, each identified by the form
# sub_XXXXXX_reconciled:
# call generate_summary_for_each_passage_with_metadata(the_parentdir_of_all_those, that_id)

summarize_errors_in_subdirectories <- function(dir_root, subfolder_match) {
  dir_root %>%
    fs::dir_ls(type = "dir", regexp = "sub-\\d+_reconciled$") %>%
    map_df(generate_summary_for_each_passage_with_metadata) # summarize all spreadsheets for that participant, for each participant
}

## Now finally: write all our results to a file (a CSV)
# annotations_base = paste(base, "derivatives/preprocessed", sep = '/')
# github_root = paste(annotations_base, "error-coding", sep = '/')

label = "disfluencies_subject-x-passage-x-word_"

outpath <- paste(sep = "", path_to_read_dataset, "/derivatives/", label, timestamp, ".csv")
# e.g. "./some/path/disfluencies_20230520_1240pm.csv"

result_megadf <-
  annotations_base %>%
  summarize_errors_in_subdirectories("^sub-\\d+_reconciled$")


write_csv(result_megadf, outpath)

print(fs::path_abs(outpath))
