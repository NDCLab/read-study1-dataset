# Reading in reconciled Excels, summarizing the errors of each, and writing that
# to a new CSV
# Luc Sahar and Jessica M. Alexander -- NDCLab, Florida International University
# last updated 2024-03-13
# This version is intended for word-level operations

### --- IO information --- ###
##  INPUTS
#     This script relies on two external stateful components:
#       - scaffolds
#       - passage annotations
#
#     Scaffolds are stored in an Excel file with one sheet per passage. Each
#     sheet contains a list of each syllable and a list of each word, aligned
#     such that it is apparent which syllables belong to which word.
#     These are autogenerated by the genScaffolds script in the read-study1
#     repository.
#
#     For our purposes, this is important for calculating word-level errors (as
#     in "word stress errors", which we have considered to be errors that affect
#     whole words rather than individual errors), for determining whether errors
#     marked on adjacent syllables constitute discrete errors or just one error
#     (as in misproductions, which we here consider to be discrete when they
#     don't touch or when they touch at a word boundary, but not when they touch
#     inside a word), and for calculating rates of errors per syllable or per
#     word (the scaffolds make it simple to count the words or syllables that
#     each passage has).
#
#     Each passage annotation is a single Excel file consisting of rows with the
#     error types and two header rows---one for the passage's words, and another
#     below it for the passage's syllables. Below that, each cell thus
#     represents the presence or absence of an error of a particular type on a
#     particular syllable in the text. Because we're interested in calculations
#     by error type, we will transpose each annotation once it is read into a
#     dataframe, such that its columns are the error types and its rows are the
#     syllables.
#
#
##  PARAMETERS
#     This script uses a couple of repeat conventions. Hopefully it is as clear
#     and consistent as intended. Some clarifications:
#       -  path: a fully specified filesystem path to a file
#       -  dir: likewise, but for a directory (folder)
#       - "name": often used to distinguish whether we're talking about the text
#                 label -- the string identifying a file -- rather than the file
#                 itself or its contents
#       - df: dataframe, the R structure
#       - passage_df: a dataframe for a participant's reading of a given
#                 passage, with that passage's scaffolding included
#       - DEBUG_MODE: a 'flag' which when set to TRUE enables features including
#                 increased verbosity and incremental outputs (in case the
#                 program fails before creating the intended file per below)
#       - incremental_writeout: the name of the directory to which incremental
#                 outputs will periodically be saved in debug mode
#       - error_types_idiomatic: R-/dataframe- friendly names of the error types
#       - dict: a dictionary, used here for storing the scaffolds, the word
#               counts per passage, and the syllable counts per passage. This
#               prevents this from having to be recomputed for >1000 passages.
#               This is implemented as a new environment, but the idea of a
#               dictionary is familiar and the syntax of accessing an `env` in
#               R is similar to that of dictionaries in languages like Python.
#               Another option would be memoizing the function.
#       - lookbacks: columns appended to a df representing, at each row, the
#               value of some other column X rows back. This appears to be the
#               best way to perform sophisticated comparisons across errors over
#               a range of previous syllables.
#               For example, we might want to look for all misproductions,
#               insertions, and omissions occurring within four syllables of a
#               following hesitation or elongation---one way we might want to
#               look for potential cases of post-error slowing.
#
#     Everything else is intended to be as simple and self-explanatory as
#     possible. The spirit of the style is to have functions that are modular,
#     digestible, and whose titles are informative enough that reading the body
#     is typically unnecessary. Ideally, this makes code easier to understand
#     and to maintain.
#
#
##  OUTPUT
#     This script, when debug mode is not turned on (see above), writes only one
#     file: a massive CSV containing one row per participant per passage (there
#     are 20 rows per participant, assuming they read all passages).
#     Specific directories (rather than parameter names) are named with "base";
#     this was arbitrary.
#
#     That CSV is written to `outpath`, which is currently the concatenation of
#     the annotations root directory `annotations_base`, a somewhat useful
#     file name prefix (`label`), the current date and time (`timestamp`), and
#     the file extension '.csv'.
#
#     That file name is the last value returned if the script has run through in
#     its entirety. Other information is emitted by R along the way, along with
#     some other status updates when debug mode is turned on.
##
### --- IO information --- ###

DEBUG_MODE = TRUE

library(readxl) # read_xlsx
library(stringr) # str_extract
library(dplyr) # most things
library(purrr) # map, map_df; generally good to have
library(lubridate) # now
library(readr) # write_csv
library(glue) # glue

# setwd("your/path/to/this/repo/../read-study1-analysis")
# We need a directory with the gold standard for each of these passages:
path_to_read_dataset <- "." # "../read-study1-dataset"
# path_to_error_coding_excels <-
#   paste(path_to_read_dataset,
#         "materials/reading-ranger/stimuli/passage-error-excels",
#         sep = '/')
# this makes one assumption, namely that the `read-study1-dataset` and
# `read-study1-analysis` repos are located in the same parent folder

# fs::dir_exists(path_to_error_coding_excels)

# passage_dir = "~/Documents/ndclab/rwe-analysis-sandbox/github-structure-mirror/readAloud-valence-dataset/derivatives/preprocessed/error-coding/test"

default_suffix <- "_reconciled" # for participant folder names

# if we're in debug mode, write output dataframes to disk as they are made
incremental_writeout <- if(DEBUG_MODE)
  paste(base, "incremental-passages_debugging", sep = '/') else
    NULL

if(DEBUG_MODE && !fs::is_dir(incremental_writeout))
  stop(
    paste("Intended debugging CSV directory (", incremental_writeout, ") not found. Try creating it?", sep = ""))
# fs::dir_create(incremental_writeout)

filler = data.frame( # what we'll use when data is empty or invalid, until the files are manually fixed
  logical(12), # FALSE 12 times
  row.names = error_types_idiomatic # fixme
) %>% t %>% as.data.frame







## Calculations about the passages themselves, for things like word ratios
timestamp = now("America/New_York") %>% format("%Y%m%d_%I%M%P")
# stim_char <- paste(base, 'materials/readAloud-ldt/stimuli/readAloud/readAloud-stimuli_characteristics.xlsx', sep="/", collapse=NULL)
# SUBList <- paste(base, 'materials/readAloud-ldt/stimuli/resources/SUBTLEXus74286wordstextversion.txt', sep="/") #downloaded from https://www.ugent.be/pp/experimentele-psychologie/en/research/documents/subtlexus on 06/13/2022
# subtlexus <- read.table(SUBList, header=TRUE)
# subtlexus$Word <- tolower(subtlexus$Word) #make all entries in SUBTLEXUS lower-case
# fixme:
annotations_base <- paste(base, "..", "error-coding", "my-initial-annotations-for-preliminary-preproc-testing", sep = '/')

# programmatcially determine passage names
# fs::dir_ls(annotations_base, recurse = TRUE, type = "file") %>%
#   str_extract("([a-z]+)_([0-9]+g)", group = 1) %>%
#   unique() %>%
#   discard(is.na) %>%
#   sort  # %>% paste(collapse = "', '")
#
# fs::dir_ls(annotations_base, recurse = TRUE, type = "file") %>%
#   str_extract("([a-b]/[a-z]+)_([0-9]+g)", group = 1) %>%
#   unique() %>%
#   discard(is.na) %>%
#   sort %>% paste(collapse = "', '")

# titles_a <- c(
#   'boots', 'elephant', 'muffins', 'pencils', 'pine',
#   'raspberries', 'roses', 'sparrows', 'tables', 'trucks'
# )
#
# titles_b <- c(
#   'blackberries', 'cars', 'chairs', 'cupcakes', 'finches',
#   'giraffe', 'oak', 'pens', 'sandals', 'tulips'
# )
#
# titles = c('blackberries', 'boots', 'cars', 'chairs', 'cupcakes',
#            'elephant', 'finches', 'giraffe', 'muffins', 'oak',
#            'pencils', 'pens', 'pine', 'raspberries', 'roses',
#            'sandals', 'sparrows', 'tables', 'trucks', 'tulips')




into_dict <- function(sequence, f, env = new.env()) {
  map(sequence, \(x) env[[x]] = f(x)) # fill a dictionary that maps x -> f(x)
  return(env)
}

# First, we want to be able to quickly, easily, and expressively check, for any
# given passage, which group (a or b) that passage belongs to
# yield <- function(result) {
#   # give me a function that, irrespective of its arguments, returns result
#   function(...) result
# }


# membership <- into_dict(titles_a, \(.) "a")
# membership <- into_dict(titles_b, \(.) "b", env = membership)

tally_up <- function(df, col) # how many unique values in col?
  df %>% select({{col}}) %>% unique %>% nrow

# id_to_int <- function(scaffolds_df)
#   # Instead of using labels like "skunkowlsyllable1" etc, just take the int part
#   mutate(scaffolds_df,
#          across(syllable_id:word_id, ~ as.numeric(str_extract(., "\\d+"))))

# get all scaffolds as a dict, converting syllable_ and word_id into ints
# scaffolds       = into_dict(titles, \(x)
#                             read_xlsx(scaffolds_path, sheet = x) %>% id_to_int) # syntax: scaffolds[[passage_name]]   -> scaffold df for that passage
# word_counts     = into_dict(titles, \(x) tally_up(scaffolds[[x]], word_id))     # syntax: word_counts[[passage_name]] -> number of words in that passage
# syllable_counts = into_dict(titles, \(x) tally_up(scaffolds[[x]], syllable_id))
# word_lists      = into_dict(titles, \(x)
#                             read_xlsx(stim_char, sheet = x, skip = 1)[,1:2]$stimWord)

## Now: logic to read in the error passage XLSXes
build_participant_dirname <- function(dir_root, participant_id, suffix = default_suffix) # github_root, 150077 -> "/home/[...]/sub-150077/sub-150077_reconciled"
  paste(sep = "",
        dir_root,
        '/sub-', participant_id,
        '/sub-', participant_id, suffix)


find_participant_dirname <- function(dir_root, participant_id) {
  dir_root %>%
    fs::dir_ls(recurse = TRUE,
               type = "dir",
               glob = glue("/sub-{participant_id}[^/*]/"))
}

find_participant_dirname <- function(dir_root, participant_id) {
  dir_root %>%
    fs::dir_map(identity, recurse = TRUE) %>%
    str_extract(glue(".*/sub-{participant_id}[^/]+/")) %>%
    discard(is.na) %>%
    unique()
}

find_participant_dirname(annotations_base, 3200002)


build_full_passage_path <- function(participant_dir_root, passage_nickname, grade) {
  participant_dir_root %>%
    paste('passage-set-', grade, membership[[passage_nickname]], '/', sep = '') %>%
    paste(passage_nickname, '_', grade, 'g.xlsx', sep = '')

} # %>% fs::is_file()

read_error_data_from_path <- function(passage_path) {
  df = data.frame(
    read_xlsx(passage_path)[2:13,], # get only the rows misprod ... corrected
    row.names = error_types_idiomatic
  ) %>% t

  return(df[-1,] %>% # ignore the original titles
           as.data.frame) # and convert back to a dataframe
}


passage_name_to_df <- function(passage_nickname, participant_id, dir_root)
  cbind(scaffolds[[passage_nickname]], # include our scaffolds
        passage_nickname %>%
          build_full_passage_path(dir_root, participant_id) %>% # path
          read_error_data_from_path
  )

complain_when_invalid <- function(passage_df, participant_id, passage_nickname) {
  report = paste("\n\t\t<< ERROR REPORT", participant_id, "-", passage_nickname, ">>")

  any_empty = sum(is.na(passage_df)) != 0
  if (any_empty) {
    message(report); message("Empty value (NA) in the dataframe!\n")
    return(filler) # 7 values of FALSE
  }

  any_invalid = any(passage_df !=0 & passage_df != 1)
  if(any_invalid) {
    message(report); message("Invalid value (neither 1 nor 0) in the dataframe!\n")
    return(filler) # 7 values of FALSE
  }

  return(passage_df)
}


collapse_by_word <- function(passage_df) {
  # for each word in the passage, consider it an error when any of its syllables
  # is marked as such

  # map over words
  # is any syllable corresponding to this word marked?
  # if yes TRUE
  # else FALSE

  passage_df %>%
    mutate(across(misprod:corrected,
                  ~ as.logical(as.integer(.)))) %>% # make booleans
    group_by(word_id) %>%                           # collapse as words
    summarize(across(misprod:corrected, ~ any(.)))  # error on this word or not?
}


get_frequency_for_word <- function(word) {
  # caution: if a lemma is desired, it must be derived elsewhere (this function
  # does not contain that logic)
  return(subtlexus$Lg10WF[match(word, subtlexus$Word)] %>%
           replace(is.na(.), 0))
}


get_frequencies_for_passage <- function(nickname)
  word_lists[[nickname]] %>% map_vec(get_frequency_for_word)

passage_frequencies = into_dict(titles, get_frequencies_for_passage)

append_words_and_frequencies <- function(collapsed_df, nickname)
  cbind(word           = word_lists[[nickname]],
        log10frequency = passage_frequencies[[nickname]],
        collapsed_df)

colnames_from_range <- function(df, colrange)
  colnames(select(df, {{colrange}}))

append_lookback <- function(df, col, lookback_index)
  # Add a new column (e.g. prev_misprod4) representing the value of e.g. misprod, four rows prior.
  # This is useful for hunting for patterns of errors in a particular sequence
  mutate(df,
         "prev_{col}{lookback_index}" := lag(df[[col]], n = lookback_index))

append_lookback_multicol <- function(df, colrange, lookback_index) {
  # for every column in passed range, create a new column looking back at the indexth row for that column
  col_list = colnames_from_range(df, {{colrange}})

  reduce(col_list,
         \(df_acc, colname) append_lookback(df_acc, {{colname}}, lookback_index),
         .init = df)
}

append_lookbacks_multicol <- function(df, colrange, lookback_count)
  # Use append_lookback_multicol successively on the same df, on each index from 1..lookback_count
  # Ex: col=hesitation, lookback_count=3 will add a column for hesitations three rows prior,
  # another for hesitations two rows prior, and another for hesitations one row prior
  reduce(1:lookback_count,
         partial(append_lookback_multicol, colrange = {{colrange}}),
         .init = df)


a_b_sequence_lookback <- function(df, errtypes_a, errtypes_b, prior_context = 1) {
  # a: LHS errors; b: RHS errors; context: how many rows back we can look for LHS errors

  lhs_cols = colnames_from_range(df, {{errtypes_a}})
  rhs_cols = colnames_from_range(df, {{errtypes_b}})

  lookbacks_regex = lhs_cols %>% paste(collapse = "|") %>% paste("prev_(", ., ").*", sep = "") # as in prev_(misprod|hesitation).*

  df_with_lhs_lookbacks = append_lookbacks_multicol(df, {{errtypes_a}}, prior_context)


  mutate(df_with_lhs_lookbacks,
         "any_prior_{{errtypes_a}}" := if_any(matches(lookbacks_regex), ~ . == 1),
         "{{errtypes_b}}_with_any_prior_{{errtypes_a}}" := if_any(rhs_cols, ~ . == 1) & if_any(matches(lookbacks_regex), ~ . == 1))
}

# All the above, with lookaheads
append_lookahead <- function(df, col, lookahead_index)
  # Add a new column (e.g. next_misprod4) representing the value of e.g. misprod, four rows ahead.
  # This is useful for hunting for patterns of errors in a particular sequence
  mutate(df,
         "next_{col}{lookahead_index}" := lead(df[[col]], n = lookahead_index))

append_lookahead_multicol <- function(df, colrange, lookahead_index) {
  # for every column in passed range, create a new column looking forward at the indexth row for that column
  col_list = colnames_from_range(df, {{colrange}})

  reduce(col_list,
         \(df_acc, colname) append_lookahead(df_acc, {{colname}}, lookahead_index),
         .init = df)
}

append_lookaheads_multicol <- function(df, colrange, lookahead_count)
  # Use append_lookahead_multicol successively on the same df, on each index from 1..lookahead_count
  # Ex: col=hesitation, lookahead_count=3 will add a column for hesitations three rows ahead,
  # another for hesitations two rows ahead, and another for hesitations one row ahead
  reduce(1:lookahead_count,
         partial(append_lookahead_multicol, colrange = {{colrange}}),
         .init = df)


a_b_sequence_lookahead <- function(df, errtypes_a, errtypes_b, forward_context = 1) {
  # a: LHS errors; b: RHS errors; context: how many rows forward we can look for RHS errors

  lhs_cols = colnames_from_range(df, {{errtypes_a}})
  rhs_cols = colnames_from_range(df, {{errtypes_b}})

  lookaheads_regex = rhs_cols %>% paste(collapse = "|") %>% paste("next_(", ., ").*", sep = "") # as in next_(misprod|hesitation).*

  df_with_rhs_lookaheads = append_lookaheads_multicol(df, {{errtypes_b}}, forward_context)

  mutate(df_with_rhs_lookaheads,
         "any_upcoming_{{errtypes_b}}" := if_any(matches(lookaheads_regex), ~ . == 1),
         "{{errtypes_a}}_with_any_upcoming_{{errtypes_b}}" := if_any(lhs_cols, ~ . == 1) & if_any(matches(lookaheads_regex), ~ . == 1)
  )
}

# we just want from lookaheads and lookbacks:
# was there ANY in previous 5
# psuedocode: any(prev_*)
# likewise for next

# & all of this for misprod->hes and hes->misprod

# so we add four columns:
# is it a misprod with preceding hes?
# is it a misprod with following hes?
# is it a hes with preceding misprod?
# is it a hes with following misprod?

append_pes_annotation_cols <- function(df, errtypes_a, errtypes_b, forward_context = 1, prior_context = 1) {
  # a: LHS errors; b: RHS errors; context: how many rows forward we can look for RHS errors

  lhs_cols = colnames_from_range(df, {{errtypes_a}})
  rhs_cols = colnames_from_range(df, {{errtypes_b}})

  lookaheads_regex = rhs_cols %>% paste(collapse = "|") %>% paste("next_(", ., ").*", sep = "") # as in next_(misprod|hesitation).*
  df_with_rhs_lookaheads = append_lookaheads_multicol(df, {{errtypes_b}}, forward_context)

  lookbacks_regex = lhs_cols %>% paste(collapse = "|") %>% paste("prev_(", ., ").*", sep = "") # as in prev_(misprod|hesitation).*
  df_with_lhs_lookbacks = append_lookbacks_multicol(df, {{errtypes_a}}, prior_context)

  df_with_rhs_lookaheads_and_lhs_lookbacks =
    df %>%
    a_b_sequence_lookahead({{errtypes_a}}, {{errtypes_b}}, forward_context) %>%
    a_b_sequence_lookback({{errtypes_a}}, {{errtypes_b}}, prior_context)

  return(df_with_rhs_lookaheads_and_lhs_lookbacks)
}

status_message <- function(passage_name, participant_id) {
  status = paste("Generating word errors from participant ", participant_id, "'s ",
                 passage_name, " passage...",
                 sep = '')
  message(status)
}

error_summary_with_metadata <- function(passage_nickname, participant_id, dir_root) {
  if(DEBUG_MODE) status_message(passage_nickname, participant_id)

  summary =
    passage_name_to_df(passage_nickname, participant_id, dir_root) %>%
    # complain_when_invalid(participant_id, passage_nickname) %>%
    collapse_by_word %>%
    append_words_and_frequencies(passage_nickname) %>%
    append_pes_annotation_cols(misprod, hesitation, forward_context = 5, prior_context =  5) %>%
    append_pes_annotation_cols(hesitation, misprod, forward_context = 5, prior_context =  5) %>%
    select(word:corrected, matches("(^|.*_)any.*[_$]")) # any_prior_misprod, any_following_hesitation, any_prior_hesitation, any_following_misprod)

  return(cbind(
    id = participant_id, # pre-pose an id column
    passage = passage_nickname, # then a passage column
    summary
  ))
}

# All passages for a participant
generate_summary_for_each_passage_with_metadata <- function(dir_root, participant_id, write_to = incremental_writeout) {
  df =
    build_participant_dirname(dir_root, participant_id) %>% # identify their folder
    dir %>% # pick out: which passages did they actually read?
    fs::path_ext_remove() %>% # take them _without_ the extension
    map_df(error_summary_with_metadata, participant_id, dir_root) # glue that together into a df

  if (!is.null(write_to) && fs::is_dir(write_to)) { # incremental CSV: just this participant, with all their passages
    outfile_debug = paste(write_to, "/", participant_id, "_", timestamp, '.csv', sep = "")
    write_csv(df, outfile_debug)
  }

  return(df)
}

# Now, for each participant under a directory, each identified by the form sub_XXXXXX_reconciled,
# call generate_summary_for_each_passage_with_metadata(the_parentdir_of_all_those, that_id)
summarize_errors_in_subdirectories <- function(dir_root, subfolder_match)
  dir_root %>%
  dir(include.dirs = TRUE, recursive = TRUE, pattern = subfolder_match) %>% # walk the directory
  map(\(dir) str_extract(dir,  "\\d+")) %>% # split them up: sub-150079_reconciled -> 150079
  map_df(generate_summary_for_each_passage_with_metadata, dir_root = dir_root) # summarize all spreadsheets for that participant, for each participant

# we've matched subfolders by explicitly returning directories (include.dirs =
# TRUE) and recursing (recursive = TRUE), thus catching -"_reconciled" subfolders

## Now finally: write all our results to a file (a CSV)
annotations_base = paste(base, "derivatives/preprocessed", sep = '/')
github_root = paste(annotations_base, "error-coding", sep = '/')

label = "disfluencies_subject-x-passage-x-word_"

outpath <- paste(sep = "", annotations_base, '/', label, timestamp, ".csv")
# e.g. "./some/path/disfluencies_20230520_1240pm.csv"

github_root %>% # passage_dir %>%
  summarize_errors_in_subdirectories("^sub-\\d{6}_reconciled$") %>%
  write_csv(outpath)

print(outpath)
